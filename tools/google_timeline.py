import json
import pandas as pd
from datetime import datetime, timedelta
import streamlit as st
import io


def parse_timestamp(time_str, offset):
    if time_str:
        dt = datetime.fromisoformat(time_str.replace('Z', '+00:00'))
        return dt - timedelta(minutes=offset)
    return None


def extract_timeline_data(uploaded_file):
    """JSONãƒ‡ãƒ¼ã‚¿ã‚’è§£æã—ã€DataFrameã«å¤‰æ›ï¼ˆè»½é‡åŒ–å¯¾å¿œï¼‰"""
    file_content = uploaded_file.getvalue().decode()  # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–ã®ãŸã‚ã«å¤‰æ›
    data = json.load(io.StringIO(file_content))  # æ–‡å­—åˆ—ã¨ã—ã¦å‡¦ç†
    if 'semanticSegments' not in data:
        raise ValueError("Error: 'semanticSegments' key not found in the JSON file.")

    records = []
    for segment in data['semanticSegments']:
        start_time = parse_timestamp(segment.get('startTime'), segment.get('startTimeTimezoneUtcOffsetMinutes', 0))
        end_time = parse_timestamp(segment.get('endTime'), segment.get('endTimeTimezoneUtcOffsetMinutes', 0))

        if 'timelinePath' in segment:
            for path in segment['timelinePath']:
                point_time = parse_timestamp(path.get('time'), segment.get('startTimeTimezoneUtcOffsetMinutes', 0))
                lat, lng = map(float, path['point'].replace('Â°', '').split(', '))
                records.append(["timelinePath", start_time, end_time, point_time, lat, lng, None, None, None, None, None, None])

        if 'visit' in segment:
            visit = segment['visit']
            top_candidate = visit.get('topCandidate', {})
            if 'placeLocation' in top_candidate and 'latLng' in top_candidate['placeLocation']:
                lat, lng = map(float, top_candidate['placeLocation']['latLng'].replace('Â°', '').split(', '))
                records.append(["visit", start_time, end_time, None, lat, lng, visit.get('probability'), top_candidate.get('placeId'), top_candidate.get('semanticType'), None, None, None])

        if 'activity' in segment:
            activity = segment['activity']
            top_candidate = activity.get('topCandidate', {})
            for key in ['start', 'end']:
                if key in activity and 'latLng' in activity[key]:
                    lat, lng = map(float, activity[key]['latLng'].replace('Â°', '').split(', '))
                    records.append([f"activity_{key}", start_time, end_time, None, lat, lng, None, None, None, activity.get('distanceMeters'), top_candidate.get('type'), top_candidate.get('probability')])

    columns = ["type", "start_time", "end_time", "point_time", "latitude", "longitude", "visit_probability", "visit_placeId", "visit_semanticType", "activity_distanceMeters", "activity_type", "activity_probability"]
    df = pd.DataFrame(records, columns=columns)

    # ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–: floatå‹ã‚’float32ã«å¤‰æ›
    df["latitude"] = df["latitude"].astype("float64")
    df["longitude"] = df["longitude"].astype("float64")
    df["activity_distanceMeters"] = pd.to_numeric(df["activity_distanceMeters"], errors="coerce").astype("float32")
    df["visit_probability"] = pd.to_numeric(df["visit_probability"], errors="coerce").astype("float32")
    df["activity_probability"] = pd.to_numeric(df["activity_probability"], errors="coerce").astype("float32")

    return df


# Streamlit UI
def google_timeline():
    st.title("ğŸ“ Google Timeline JSON Converter")

    # èª¬æ˜
    st.markdown("""
    **ã“ã®ãƒ„ãƒ¼ãƒ«ã«ã¤ã„ã¦**  
    Google ãƒãƒƒãƒ—ã® **ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ JSON ãƒ•ã‚¡ã‚¤ãƒ«** ã‚’ã€è¦‹ã‚„ã™ã„è¡¨å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚  
    CSVã¨ã—ã¦ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸã‚Šã€åœ°å›³ä¸Šã«è¡¨ç¤ºã—ãŸã‚Šã§ãã¾ã™ã€‚
    """)
    

    png_path = "items/googel_timeline-1.png"
    st.image(png_path, caption="Googleãƒãƒƒãƒ—ã®ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ–¹æ³•", use_container_width=True)

    # JSONã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
    uploaded_file = st.file_uploader("ğŸ“‚ ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type="json")

    if uploaded_file is not None:
        with st.spinner("ğŸ”„ ãƒ‡ãƒ¼ã‚¿ã‚’è§£æä¸­..."):
            df = extract_timeline_data(uploaded_file)

        st.success(f"âœ… è§£æå®Œäº†ï¼{len(df):,} è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã—ã¾ã—ãŸã€‚")

        
        st.subheader("ğŸ“Š è§£æçµæœ (æœ€å¤§50è¡Œè¡¨ç¤º)")
        st.dataframe(df.head(50))  

        csv_data = df.to_csv(index=False).encode('utf-8')

        st.download_button("ğŸ“¥ CSVã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", csv_data, "timeline_data.csv", "text/csv")
        
        
        st.subheader("ğŸ“Š å„ã‚«ãƒ©ãƒ ã®èª¬æ˜")
        st.markdown("""
        | **ã‚«ãƒ©ãƒ å**                | **èª¬æ˜**                                                                                                                                     |
        |-----------------------------|----------------------------------------------------------------------------------------------------------------------------------------------|
        | **type**                    | ã‚¤ãƒ™ãƒ³ãƒˆã®ç¨®é¡ã€‚`timelinePath`ï¼ˆç§»å‹•çµŒè·¯ï¼‰ã€`visit`ï¼ˆè¨ªå•å ´æ‰€ï¼‰ã€`activity_start` / `activity_end`ï¼ˆã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã®é–‹å§‹/çµ‚äº†ï¼‰ãŒã‚ã‚Šã¾ã™ã€‚ |
        | **start_time**               | ã‚¤ãƒ™ãƒ³ãƒˆã®é–‹å§‹æ™‚åˆ»ï¼ˆISO 8601å½¢å¼ã€ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãï¼‰ã€‚                                                                                       |
        | **end_time**                 | ã‚¤ãƒ™ãƒ³ãƒˆã®çµ‚äº†æ™‚åˆ»ï¼ˆISO 8601å½¢å¼ã€ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ä»˜ãï¼‰ã€‚                                                                                       |
        | **point_time**               | ç‰¹å®šã®ã‚¿ã‚¤ãƒ ãƒã‚¤ãƒ³ãƒˆã®æ™‚åˆ»ï¼ˆ`NaT`ã¯æ™‚åˆ»ãŒå­˜åœ¨ã—ãªã„ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼‰ã€‚                                                                        |
        | **latitude**                 | ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã—ãŸå ´æ‰€ã®ç·¯åº¦ï¼ˆä¾‹: 34.780489ï¼‰ã€‚                                                                                               |
        | **longitude**                | ã‚¤ãƒ™ãƒ³ãƒˆãŒç™ºç”Ÿã—ãŸå ´æ‰€ã®çµŒåº¦ï¼ˆä¾‹: 135.475209ï¼‰ã€‚                                                                                              |
        | **visit_probability**        | è¨ªå•ã®ç¢ºç‡ã€‚è¨ªå•ã—ã¦ã„ã‚‹å ´åˆã®ç¢ºç‡ï¼ˆ`NaN`ã¯ãƒ‡ãƒ¼ã‚¿ãŒãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ï¼‰ã€‚                                                                 |
        | **visit_placeId**            | è¨ªå•ã—ãŸå ´æ‰€ã®IDï¼ˆGoogle Places APIã®IDãªã©ï¼‰ã€‚                                                                                               |
        | **visit_semanticType**       | è¨ªå•ã—ãŸå ´æ‰€ã®ã‚¿ã‚¤ãƒ—ï¼ˆä¾‹: `INFERRED_HOME`ã€`RESTAURANT`ï¼‰ã€‚                                                                                   |
        | **activity_distanceMeters**  | ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã®ç§»å‹•è·é›¢ï¼ˆãƒ¡ãƒ¼ãƒˆãƒ«å˜ä½ï¼‰ã€‚`NaN`ã¯ç§»å‹•ãŒè¨˜éŒ²ã•ã‚Œã¦ã„ãªã„ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚                                                   |
        | **activity_type**            | ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã®ã‚¿ã‚¤ãƒ—ï¼ˆä¾‹: `UNKNOWN_ACTIVITY_TYPE`ã€`WALKING`ã€`DRIVING`ï¼‰ã€‚                                                      |
        | **activity_probability**     | ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ã®ç¢ºç‡ï¼ˆ0.0ã¯ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãŒç™ºç”Ÿã—ã¦ã„ãªã„ã“ã¨ã‚’ç¤ºã—ã€1.0ã¯ã‚¢ã‚¯ãƒ†ã‚£ãƒ“ãƒ†ã‚£ãŒç¢ºå®Ÿã§ã‚ã‚‹ã“ã¨ã‚’ç¤ºã—ã¾ã™ï¼‰ã€‚                        |
        """)
        
        if "latitude" in df.columns and "longitude" in df.columns:
            st.subheader("ğŸ“ ä½ç½®æƒ…å ±ã‚’åœ°å›³ã«è¡¨ç¤º")

            # **ãƒ‡ãƒ¼ã‚¿ãŒå¤šã™ãã‚‹å ´åˆã€ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°**
            if len(df) > 10_0000:
                st.warning("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒå¤šã„ãŸã‚ã€ä¸€éƒ¨ã®ã¿è¡¨ç¤ºã—ã¦ã„ã¾ã™ (10,0000ä»¶ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°)")
                df_map = df.sample(10_0000, random_state=42)
            else:
                df_map = df.copy()

            st.map(df_map[["latitude", "longitude"]])
