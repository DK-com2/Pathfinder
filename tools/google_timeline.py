import streamlit as st
import pandas as pd
import numpy as np
import io
import json
from datetime import datetime, timedelta
import psycopg2
from psycopg2.extras import execute_values
from supabase import create_client, Client

# Supabase ã®ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
SUPABASE_URL = st.secrets["supabase"]["url"]
SUPABASE_KEY = st.secrets["supabase"]["key"]
supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)


def show_username_if_exists(supabase):
    # ã‚»ãƒƒã‚·ãƒ§ãƒ³ã‹ã‚‰ user_id ã‚’å–å¾—
    try:
        user_dict = st.session_state.user.dict()
        user_id = user_dict.get("id", None)
    except AttributeError:
        st.warning("âš  ãƒ¦ãƒ¼ã‚¶ãƒ¼æƒ…å ±ãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None
    
    if not user_id:
        st.warning("âš  ãƒ¦ãƒ¼ã‚¶ãƒ¼IDãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
        return None

    # Supabaseã‹ã‚‰ username ã‚’å–å¾—
    response = supabase.table("username").select("username").eq("user_id", user_id).execute()

    if response.data:
        username = response.data[0]["username"]
        st.session_state.username = username  # â† ã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ï¼
        st.markdown(f"**ãƒ¦ãƒ¼ã‚¶ãƒ¼å**: {username}")
        return username
    else:
        st.warning("âš  ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒç™»éŒ²ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return None


def convert_series_to_utc(series: pd.Series) -> pd.Series:
    dt_series = pd.to_datetime(series, errors='coerce')
    if dt_series.dt.tz is None:
        dt_series = dt_series.dt.tz_localize('Asia/Tokyo')
    else:
        dt_series = dt_series.dt.tz_convert('Asia/Tokyo')
    return dt_series.dt.tz_convert('UTC')


def extract_timeline_data(uploaded_file):
    file_content = uploaded_file.getvalue().decode()
    data = json.load(io.StringIO(file_content))

    if 'semanticSegments' not in data:
        raise ValueError("Error: 'semanticSegments' key not found in the JSON file.")

    records = []
    for segment in data['semanticSegments']:
        start_time = segment.get('startTime')
        end_time = segment.get('endTime')

        if 'timelinePath' in segment:
            for path in segment['timelinePath']:
                point_time = path.get('time')

                try:
                    lat, lng = map(float, path['point'].replace('Â°', '').split(', '))
                except Exception:
                    lat, lng = None, None

                records.append([
                    "timelinePath", start_time, end_time, point_time, lat, lng,
                    None, None, None, None, None, None, st.session_state.username
                ])

        if 'visit' in segment:
            visit = segment['visit']
            top_candidate = visit.get('topCandidate', {})
            try:
                lat, lng = map(float, top_candidate.get('placeLocation', {}).get('latLng', '').replace('Â°', '').split(', '))
            except Exception:
                lat, lng = None, None

            records.append([
                "visit", start_time, end_time, None, lat, lng,
                visit.get('probability'), top_candidate.get('placeId'), top_candidate.get('semanticType'),
                None, None, None, st.session_state.username
            ])

        if 'activity' in segment:
            activity = segment['activity']
            top_candidate = activity.get('topCandidate', {})
            for key in ['start', 'end']:
                if key in activity and 'latLng' in activity[key]:
                    try:
                        lat, lng = map(float, activity[key]['latLng'].replace('Â°', '').split(', '))
                    except Exception:
                        lat, lng = None, None

                    records.append([
                        f"activity_{key}", start_time, end_time, None, lat, lng,
                        None, None, None,
                        activity.get('distanceMeters'), top_candidate.get('type'), top_candidate.get('probability'),
                        st.session_state.username
                    ])

    columns = [
        "type", "start_time", "end_time", "point_time", "latitude", "longitude",
        "visit_probability", "visit_placeId", "visit_semanticType",
        "activity_distanceMeters", "activity_type", "activity_probability",
        "username"
    ]
    df = pd.DataFrame(records, columns=columns)

    # â± ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—åˆ—ã®é«˜é€Ÿä¸€æ‹¬å¤‰æ›
    for col in ["start_time", "end_time", "point_time"]:
        df[col] = convert_series_to_utc(df[col])

    df = df.astype({
        "latitude": "float64",
        "longitude": "float64",
        "activity_distanceMeters": "float32",
        "visit_probability": "float32",
        "activity_probability": "float32"
    })

    df = df.replace({np.nan: None})
    return df


def upload_to_postgresql(df, conn, table_name="timeline_data"):
    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰å‡¦ç†ä¸­ã®è¡¨ç¤º
    with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã„ã¾ã™... å°‘ã€…ãŠå¾…ã¡ãã ã•ã„ã€‚"):
        with conn.cursor() as cur:
            insert_query = f"""
            INSERT INTO {table_name} (
                type, start_time, end_time, point_time,
                latitude, longitude,
                visit_probability, visit_placeId, visit_semanticType,
                activity_distanceMeters, activity_type, activity_probability,
                username
            ) VALUES %s
            """
            values = df.values.tolist()
            execute_values(cur, insert_query, values)
            conn.commit()


def google_timeline():
    username = st.session_state.get("username", None)
    if not username:
        st.warning("âš  ãƒ¦ãƒ¼ã‚¶ãƒ¼åãŒã‚»ãƒƒã‚·ãƒ§ãƒ³ã«ä¿å­˜ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚")
        return

    st.title("ğŸ“ Google Timeline ãƒ‡ãƒ¼ã‚¿ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")

    # èª¬æ˜ã¨ç”»åƒã‚’è¿½åŠ 
    st.markdown("### â“ Google ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ–¹æ³•")
    st.markdown("""
    ã¾ãšã€Google ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã¯ã‚¹ãƒãƒ›ã®ã€Œä½ç½®æƒ…å ±ã€æ©Ÿèƒ½ã‚’ä½¿ã£ã¦ç«¯æœ«æœ¬ä½“ã«åé›†ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚

    1. **Androidã®å ´åˆ**: è¨­å®šç”»é¢ã‹ã‚‰ã€Œä½ç½®æƒ…å ±ã€è¨­å®šã«é€²ã¿ã€ã€ŒGoogle ã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’æ¢ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚
    2. **iPhoneã®å ´åˆ**: ã¾ãã€å°‘ã—å¤§å¤‰ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ãŒã€é ‘å¼µã£ã¦æ¢ã—ã¦ã¿ã¦ãã ã•ã„ã­ã€‚Google ãƒãƒƒãƒ—ã‚¢ãƒ—ãƒªå†…ã«ã‚‚ã‚ªãƒ—ã‚·ãƒ§ãƒ³ãŒã‚ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ï¼
    3. ã€Œã‚¿ã‚¤ãƒ ãƒ©ã‚¤ãƒ³ã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã€ã‹ã‚‰**JSONå½¢å¼**ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã—ã¦ãã ã•ã„ã€‚

    ã“ã‚Œã§æº–å‚™å®Œäº†ã§ã™ï¼ã‚ã¨ã¯ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã ã‘ã§ã™ã€‚
    """)
    st.image("items/googel_timeline-1.png", caption="Google Timelineã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ‰‹é †", use_container_width=True)

    # ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰UI
    st.markdown("### ğŸ“¤ JSONãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰")
    uploaded_file = st.file_uploader("Google Timeline JSONãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠ", type=["json"])

    if uploaded_file:
        try:
            df = extract_timeline_data(uploaded_file)
            df["username"] = username  # ãƒ¦ãƒ¼ã‚¶ãƒ¼åã‚’è¿½åŠ 
            st.success(f"{len(df)} ä»¶ã®ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’æŠ½å‡ºã—ã¾ã—ãŸã€‚")
            st.dataframe(df, use_container_width=True)

            # PostgreSQLã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³
            if st.button("â¬† PostgreSQLã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰"):
                with st.spinner("ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ä¸­..."):
                    conn = psycopg2.connect(**st.secrets["postgresql"])
                    upload_to_postgresql(df, conn)
                    conn.close()
                st.success("âœ… ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ãŒå®Œäº†ã—ã¾ã—ãŸï¼")

        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")